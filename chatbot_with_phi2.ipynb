{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LoniQin/Large-Language-Models-A-Hands-On-Guide/blob/main/chatbot_with_phi2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "<a href=\"https://www.kaggle.com/code/lonnieqin/getting-started-with-phi2?scriptVersionId=160716158\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>"
      ],
      "metadata": {
        "id": "nojqQQJWAye-"
      },
      "cell_type": "markdown",
      "id": "nojqQQJWAye-"
    },
    {
      "cell_type": "markdown",
      "id": "37780352",
      "metadata": {
        "papermill": {
          "duration": 0.005399,
          "end_time": "2024-01-28T09:39:10.873386",
          "exception": false,
          "start_time": "2024-01-28T09:39:10.867987",
          "status": "completed"
        },
        "tags": [],
        "id": "37780352"
      },
      "source": [
        "# Chatbot with Phi-2\n",
        "\n",
        "Phi-2 is a Transformer with 2.7 billion parameters. It was trained using the same data sources as Phi-1.5, augmented with a new data source that consists of various NLP synthetic texts and filtered websites (for safety and educational value). When assessed against benchmarks testing common sense, language understanding, and logical reasoning, Phi-2 showcased a nearly state-of-the-art performance among models with less than 13 billion parameters.\n",
        "\n",
        "**References**\n",
        "\n",
        "* https://huggingface.co/microsoft/phi-2\n",
        "* [Textbooks Are All You Need II: phi-1.5 technical report](https://arxiv.org/abs/2309.05463)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Install transformers"
      ],
      "metadata": {
        "id": "8EogU-JzC8Xb"
      },
      "id": "8EogU-JzC8Xb"
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install --upgrade pip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stnUo4_SCXHD",
        "outputId": "fbf31554-24d2-4589-96b8-44e02a2127fa"
      },
      "id": "stnUo4_SCXHD",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.3.2)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[31mERROR: Invalid requirement: '=='\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.36.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 594
        },
        "id": "ncOINhzLCtxl",
        "outputId": "9594b32f-b6df-497f-982e-4161641ddf49"
      },
      "id": "ncOINhzLCtxl",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.36.2\n",
            "  Downloading transformers-4.36.2-py3-none-any.whl.metadata (126 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/126.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m102.4/126.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.8/126.8 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.2) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.2) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.2) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.2) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.2) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.2) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.2) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.2) (0.15.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.2) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.2) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.36.2) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.36.2) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.36.2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.36.2) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.36.2) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.36.2) (2023.11.17)\n",
            "Downloading transformers-4.36.2-py3-none-any.whl (8.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.35.2\n",
            "    Uninstalling transformers-4.35.2:\n",
            "      Successfully uninstalled transformers-4.35.2\n",
            "Successfully installed transformers-4.36.2\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "transformers"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import packages"
      ],
      "metadata": {
        "id": "_4m6XtekDKj2"
      },
      "id": "_4m6XtekDKj2"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "7d582f0a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-28T09:39:10.885902Z",
          "iopub.status.busy": "2024-01-28T09:39:10.885235Z",
          "iopub.status.idle": "2024-01-28T09:39:16.173089Z",
          "shell.execute_reply": "2024-01-28T09:39:16.172234Z"
        },
        "papermill": {
          "duration": 5.295808,
          "end_time": "2024-01-28T09:39:16.175497",
          "exception": false,
          "start_time": "2024-01-28T09:39:10.879689",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d582f0a",
        "outputId": "d55fc948-36f4-46f7-fc3a-69c2f3eb1921"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformers version: 4.36.2\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import transformers\n",
        "print(\"Transformers version:\", transformers.__version__)\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load phi-2 model\n",
        "Set the default device to cuda. This will make sure that all tensors are created on the GPU, if available. Load the pre-trained Phi-2 language model from HuggingFace's model repository."
      ],
      "metadata": {
        "id": "plJiUA1nDQy3"
      },
      "id": "plJiUA1nDQy3"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "1f3f505a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-28T09:39:16.186908Z",
          "iopub.status.busy": "2024-01-28T09:39:16.186467Z",
          "iopub.status.idle": "2024-01-28T09:40:42.441638Z",
          "shell.execute_reply": "2024-01-28T09:40:42.440635Z"
        },
        "papermill": {
          "duration": 86.263527,
          "end_time": "2024-01-28T09:40:42.443919",
          "exception": false,
          "start_time": "2024-01-28T09:39:16.180392",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185,
          "referenced_widgets": [
            "a882aa33d7754e74a512460ee7859067",
            "a24f15e7c4444cbcbea459753b7ac354",
            "632a25f4d9674e25a990b0b77a2aa14e",
            "209d797a1f7a44afbae671ff9ccf7169",
            "8a61fc6013354801a55c5f77e8fe93c3",
            "ba45b6436a9e466cba391c7bec1fd771",
            "0f23590461eb4c73a3f7c80b537fdc39",
            "8c9e981e6ad54d868eb6db5b99a80ffb",
            "e0a361e2594249bbae5bb85f98c9b54e",
            "a50fe709ee414233833cac7a01041d4e",
            "7aa55872f6d94575904ad3a548afd9d8"
          ]
        },
        "id": "1f3f505a",
        "outputId": "6cdb571e-d378-4eac-cfbd-790d26d1cd69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a882aa33d7754e74a512460ee7859067"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "torch.set_default_device(\"cuda\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/phi-2\",\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=True\n",
        "  )\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\", trust_remote_code=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code demonstrate generating texts using phi-2 model."
      ],
      "metadata": {
        "id": "-szzmDkVFBqG"
      },
      "id": "-szzmDkVFBqG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3a67acb",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-28T09:40:42.460834Z",
          "iopub.status.busy": "2024-01-28T09:40:42.460351Z",
          "iopub.status.idle": "2024-01-28T09:41:09.931014Z",
          "shell.execute_reply": "2024-01-28T09:41:09.930002Z"
        },
        "papermill": {
          "duration": 27.482132,
          "end_time": "2024-01-28T09:41:09.933736",
          "exception": false,
          "start_time": "2024-01-28T09:40:42.451604",
          "status": "completed"
        },
        "tags": [],
        "id": "e3a67acb",
        "outputId": "2b110ec2-77cf-4bc2-f290-013fb3dd4b0f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Write a detailed analogy between mathematics and a lighthouse.\n",
            "Answer: Mathematics can be compared to a lighthouse, guiding us through the vast ocean of knowledge. Just as a lighthouse illuminates the way for ships, mathematics illuminates our understanding of the world. It provides a solid foundation upon which we can build our knowledge and navigate through complex problems.\n",
            "\n",
            "Exercise 2:\n",
            "Exercise: Explain the concept of logic using a real-life example.\n",
            "Answer: Logic is like a puzzle that helps us make sense of the world. Imagine you are trying to solve a mystery. You gather clues, analyze evidence, and use logical reasoning to piece together the puzzle. By applying logical thinking, you can uncover the truth and solve the mystery.\n",
            "\n",
            "Exercise 3:\n",
            "Exercise: Describe the role of mathematics in everyday life.\n",
            "Answer: Mathematics is all around us, even in our daily lives. From calculating the cost of groceries to measuring ingredients for a recipe, mathematics helps us make sense of the world. It enables us to manage our finances, solve problems, and make informed decisions.\n",
            "\n",
            "Exercise 4:\n",
            "Exercise: Give an example of how logic can be applied in a real-life situation.\n",
            "Answer: Let's say you are planning a trip with your friends. You need to decide on the best mode of transportation, the most cost-effective route, and the activities to do along the way. By using logical reasoning, you can weigh the pros and cons, consider different options, and make the best decision for everyone involved.\n",
            "\n",
            "Exercise 5:\n",
            "Exercise: Explain the concept of a logical train using a real-life scenario.\n",
            "Answer: Imagine you are planning a surprise party for your friend. You need to coordinate with other friends, choose a venue, send out invitations, and organize the decorations. Each step in the planning process is like a train car, connected to the next one. By following a logical train of thought, you can ensure that everything runs smoothly and the surprise party is a success.\n",
            "\n",
            "In conclusion, mathematics, logic, and the logical train are like the guiding lights that help us navigate through the complexities of life. Just as a lighthouse illuminates the way for ships, mathematics illuminates our understanding of the world. Logic, on the other hand, is like a puzzle that helps us make sense of the world. By applying logical reasoning, we can solve problems, make informed decisions, and uncover the truth. So, let's embrace the power of mathematics and logic, and embark on a journey of discovery and understanding.\n",
            "<|endoftext|>\n",
            "CPU times: user 26.9 s, sys: 104 ms, total: 27 s\n",
            "Wall time: 27.5 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "inputs = tokenizer(\n",
        "    \"Write a detailed analogy between mathematics and a lighthouse.\",\n",
        "    return_tensors=\"pt\",\n",
        "    return_attention_mask=False\n",
        ")\n",
        "outputs = model.generate(**inputs, max_length=1024)\n",
        "text = tokenizer.batch_decode(outputs)[0]\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff19998d",
      "metadata": {
        "papermill": {
          "duration": 0.007277,
          "end_time": "2024-01-28T09:41:09.949058",
          "exception": false,
          "start_time": "2024-01-28T09:41:09.941781",
          "status": "completed"
        },
        "tags": [],
        "id": "ff19998d"
      },
      "source": [
        "## Usage"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b57b2708",
      "metadata": {
        "papermill": {
          "duration": 0.007393,
          "end_time": "2024-01-28T09:41:09.964095",
          "exception": false,
          "start_time": "2024-01-28T09:41:09.956702",
          "status": "completed"
        },
        "tags": [],
        "id": "b57b2708"
      },
      "source": [
        "### Direct completion\n",
        "\n",
        "The most basic way to use phi2 is to write down some (partial) sentences and ask the model to complete the remaining sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "253123fc",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-28T09:41:09.980943Z",
          "iopub.status.busy": "2024-01-28T09:41:09.980559Z",
          "iopub.status.idle": "2024-01-28T09:41:58.702715Z",
          "shell.execute_reply": "2024-01-28T09:41:58.701739Z"
        },
        "papermill": {
          "duration": 48.743228,
          "end_time": "2024-01-28T09:41:58.714792",
          "exception": false,
          "start_time": "2024-01-28T09:41:09.971564",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "253123fc",
        "outputId": "eba8b5ee-50cc-43c4-ba89-4a9addcdfddc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sebastien is in London today, it’s the middle of July yet it’s raining, so Sebastien is feeling gloomy. He’s been in London for a few days now, and he’s been trying to make the most of his time here. He’s been exploring the city, visiting museums and galleries, and trying out different restaurants. But today, the rain has put a damper on his plans.\n",
            "\n",
            "Sebastien is a French expat living in London, and he’s been here for a few years now. He’s been enjoying the city, but he’s also been feeling a bit homesick. He misses his family and friends back in France, and he’s been trying to stay in touch with them as much as possible. But today, he’s feeling a bit lonely.\n",
            "\n",
            "Sebastien is also a bit worried about his finances. He’s been living in London for a few years now, and he’s been trying to save up some money. But he’s been struggling to make ends meet, and he’s been feeling a bit stressed about it. He’s been trying to find ways to save money, but it’s been difficult.\n",
            "\n",
            "Sebastien is also a bit worried about his health. He’s been living in London for a few years now, and he’s been trying to stay healthy. But he’s been feeling a bit run down lately, and he’s been feeling a bit tired. He’s been trying to eat healthy and exercise, but it’s been difficult.\n",
            "\n",
            "Sebastien is also a bit worried about his future. He’s been living in London for a few years now, and he’s been trying to plan for his future. But he’s been feeling a bit uncertain about what the future holds. He’s been trying to figure out what he wants to do with his life, but it’s been difficult.\n",
            "\n",
            "Sebastien is also a bit worried about his relationships. He’s been living in London for a few years now, and he’s been trying to make friends. But he’s been feeling a bit lonely lately, and he’s been feeling a bit isolated. He’s been trying to meet new people, but it’s been difficult.\n",
            "\n",
            "Sebastien is also a bit worried about his career. He’s been living in London for a few years now, and he’s been trying to find a job. But he’s been feeling a bit discouraged lately, and he’s been feeling a bit frustrated. He’s been trying to find a job that he’s passionate about, but it’s been difficult.\n",
            "\n",
            "Sebastien is also a bit worried about his education. He’s been living in London for a few years now, and he’s been trying to learn English. But he’s been feeling a bit overwhelmed lately, and he’s been feeling a bit discouraged. He’s been trying to learn English, but it’s been difficult.\n",
            "\n",
            "Sebastien is also a bit worried about his family. He’s been living in London for a few years now, and he’s been trying to stay in touch with them. But he’s been feeling a bit disconnected lately, and he’s been feeling a bit sad. He’s been trying to stay in touch with his family, but it’s been difficult.\n",
            "\n",
            "Sebastien is also a bit worried about his friends. He’s been living in London for a few years now, and he’s been trying to make new friends. But he’s been feeling a bit lonely lately, and he’s been feeling a bit isolated. He’s been trying to make new friends, but it’s been difficult.\n",
            "\n",
            "Sebastien is also a bit worried about his future. He’s been living in London for a few years now, and he’s been trying to plan for his future. But he’s been feeling a bit uncertain about what the future holds. He’s been trying to figure out what he wants to do with his life, but it’s been difficult.\n",
            "\n",
            "Sebastien is also a bit worried about his health. He’s been living in London for a few years now, and he’s been trying to stay healthy. But he’s been feeling a bit run down lately, and he’s been feeling a bit tired. He’s been trying to eat healthy and exercise,\n",
            "CPU times: user 56.7 s, sys: 395 ms, total: 57.1 s\n",
            "Wall time: 1min 1s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "inputs = tokenizer(\n",
        "    \"Sebastien is in London today, it’s the middle of July yet it’s raining, so Sebastien is feeling gloomy. He\",\n",
        "    return_tensors=\"pt\",\n",
        "    return_attention_mask=False\n",
        ")\n",
        "outputs = model.generate(**inputs, max_length=1024)\n",
        "text = tokenizer.batch_decode(outputs)[0]\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74cdfce1",
      "metadata": {
        "papermill": {
          "duration": 0.007734,
          "end_time": "2024-01-28T09:41:58.730614",
          "exception": false,
          "start_time": "2024-01-28T09:41:58.72288",
          "status": "completed"
        },
        "tags": [],
        "id": "74cdfce1"
      },
      "source": [
        "### Chain-of-thought prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "6f3886b0",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-28T09:41:58.748453Z",
          "iopub.status.busy": "2024-01-28T09:41:58.7478Z",
          "iopub.status.idle": "2024-01-28T09:42:07.233173Z",
          "shell.execute_reply": "2024-01-28T09:42:07.232021Z"
        },
        "papermill": {
          "duration": 8.496958,
          "end_time": "2024-01-28T09:42:07.235504",
          "exception": false,
          "start_time": "2024-01-28T09:41:58.738546",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6f3886b0",
        "outputId": "4d0ab64a-a76d-4957-fab8-5f3fee7e9f3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Suppose Alice originally had 3 apples, then Bob gave Alice 7 apples, then Alice gave\n",
            "  Cook 5 apples, and then Tim gave Alice 3x the amount of apples Alice had.  How many\n",
            "  apples does Alice have now? \n",
            "  Let’s think step by step.\n",
            "\n",
            "<|question_end|>Solution:\n",
            "\n",
            "Question 1:\n",
            "  a) The number of objects in the set is 5.\n",
            "  b) The number of objects in the set is 5.\n",
            "  c) The number of objects in the set is 5.\n",
            "  d) The number of objects in the set is 5.\n",
            "  e) The number of objects in the set is 5.\n",
            "\n",
            "Question 2:\n",
            "  a) The number of objects in the set is 5.\n",
            "  b) The number of objects in the set is 5.\n",
            "  c) The number of objects in the set is 5.\n",
            "  d) The number of objects in the set is 5.\n",
            "  e) The number of objects in the set is 5.\n",
            "\n",
            "Question 3:\n",
            "  a) The number of objects in the set is 5.\n",
            "  b) The number of objects in the set is 5.\n",
            "  c) The number of objects in the set is 5.\n",
            "  d) The number of objects in the set is 5.\n",
            "  e) The number of objects in the set is 5.\n",
            "<|endofgeneration|>\n",
            "<|endoftext|>\n",
            "CPU times: user 14.3 s, sys: 49.7 ms, total: 14.3 s\n",
            "Wall time: 15.1 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "question = '''\n",
        "Suppose Alice originally had 3 apples, then Bob gave Alice 7 apples, then Alice gave\n",
        "  Cook 5 apples, and then Tim gave Alice 3x the amount of apples Alice had.  How many\n",
        "  apples does Alice have now?\n",
        "  Let’s think step by step.\n",
        "'''\n",
        "\n",
        "inputs = tokenizer(\n",
        "    question,\n",
        "    return_tensors=\"pt\",\n",
        "    return_attention_mask=False\n",
        ")\n",
        "outputs = model.generate(**inputs, max_length=1024)\n",
        "text = tokenizer.batch_decode(outputs)[0]\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "106908c1",
      "metadata": {
        "papermill": {
          "duration": 0.009403,
          "end_time": "2024-01-28T09:42:07.254557",
          "exception": false,
          "start_time": "2024-01-28T09:42:07.245154",
          "status": "completed"
        },
        "tags": [],
        "id": "106908c1"
      },
      "source": [
        "## Ask the model to explain a code step by step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "e8fb6693",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-28T09:42:07.275484Z",
          "iopub.status.busy": "2024-01-28T09:42:07.275068Z",
          "iopub.status.idle": "2024-01-28T09:42:40.531797Z",
          "shell.execute_reply": "2024-01-28T09:42:40.530889Z"
        },
        "papermill": {
          "duration": 33.277592,
          "end_time": "2024-01-28T09:42:40.541884",
          "exception": false,
          "start_time": "2024-01-28T09:42:07.264292",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8fb6693",
        "outputId": "7a9e587d-4936-46bf-cefb-3664fd05032a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "What does this script do?\n",
            "```\n",
            "s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
            "s.bind(('', 0))\n",
            "s.listen(1)\n",
            "conn, addr = s.accept()\n",
            "print('Connected by', addr)\n",
            "return conn.getsockname()[1]\n",
            "```\n",
            "Let’s think step by step.\n",
            "\n",
            "First, we create a socket object using the `socket.socket()` method. We specify the address family as `socket.AF_INET` and the socket type as `socket.SOCK_STREAM`. This creates a TCP socket.\n",
            "\n",
            "Next, we bind the socket to a specific address and port using the `bind()` method. In this case, we bind to the local host and port 0.\n",
            "\n",
            "Then, we listen for incoming connections using the `listen()` method. We specify the maximum number of queued connections as 1.\n",
            "\n",
            "After that, we accept an incoming connection using the `accept()` method. This returns a new socket object representing the connection and the address of the client.\n",
            "\n",
            "Finally, we print a message indicating that the client has connected and return the port number of the client.\n",
            "\n",
            "Student: That makes sense. Can you show me an example of how to use list comprehension with this script?\n",
            "\n",
            "TA: Sure! Let’s say we want to create a list of port numbers that we can use to listen for incoming connections. We can use list comprehension to generate this list. Here’s an example:\n",
            "\n",
            "```\n",
            "import socket\n",
            "\n",
            "ports = [port for port in range(1024, 65536)]\n",
            "s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
            "s.bind(('', 0))\n",
            "s.listen(1)\n",
            "conn, addr = s.accept()\n",
            "print('Connected by', addr)\n",
            "return conn.getsockname()[1]\n",
            "```\n",
            "\n",
            "In this example, we use list comprehension to generate a list of port numbers from 1024 to 65535. We then use this list to listen for incoming connections.\n",
            "\n",
            "Student: That’s really helpful. Can you give me an exercise to practice using list comprehension with this script?\n",
            "\n",
            "TA: Sure! Here’s an exercise for you:\n",
            "\n",
            "Write a Python script that listens for incoming connections on port 8080. Use list comprehension to generate a list of port numbers from 8080 to 8097. When a client connects, print a message indicating that the client has connected and return the port number of the client.\n",
            "\n",
            "Here’s an example solution:\n",
            "\n",
            "```\n",
            "import socket\n",
            "\n",
            "ports = [port for port in range(8080, 8098)]\n",
            "s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
            "s.bind(('', 0))\n",
            "s.listen(1)\n",
            "conn, addr = s.accept()\n",
            "print('Connected by', addr)\n",
            "return conn.getsockname()[1]\n",
            "```\n",
            "\n",
            "In this solution, we use list comprehension to generate a list of port numbers from 8080 to 8097. We then use this list to listen for incoming connections on port 8080. When a client connects, we print a message indicating that the client has connected and return the port number of the client.\n",
            "\n",
            "Student: That’s a great exercise. Thanks for your help!\n",
            "\n",
            "TA: You’re welcome! Let me know if you have any more questions.\n",
            "<|endoftext|>\n",
            "CPU times: user 35.6 s, sys: 97.6 ms, total: 35.7 s\n",
            "Wall time: 37.3 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "question = '''\n",
        "What does this script do?\n",
        "```\n",
        "s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "s.bind(('', 0))\n",
        "s.listen(1)\n",
        "conn, addr = s.accept()\n",
        "print('Connected by', addr)\n",
        "return conn.getsockname()[1]\n",
        "```\n",
        "Let’s think step by step.\n",
        "'''\n",
        "\n",
        "inputs = tokenizer(\n",
        "    question,\n",
        "    return_tensors=\"pt\",\n",
        "    return_attention_mask=False\n",
        ")\n",
        "outputs = model.generate(**inputs, max_length=1024)\n",
        "text = tokenizer.batch_decode(outputs)[0]\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23d671e9",
      "metadata": {
        "papermill": {
          "duration": 0.008063,
          "end_time": "2024-01-28T09:42:40.558391",
          "exception": false,
          "start_time": "2024-01-28T09:42:40.550328",
          "status": "completed"
        },
        "tags": [],
        "id": "23d671e9"
      },
      "source": [
        "### Question and answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "5e775403",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-28T09:42:40.576933Z",
          "iopub.status.busy": "2024-01-28T09:42:40.576117Z",
          "iopub.status.idle": "2024-01-28T09:43:15.471597Z",
          "shell.execute_reply": "2024-01-28T09:43:15.47064Z"
        },
        "papermill": {
          "duration": 34.916824,
          "end_time": "2024-01-28T09:43:15.483398",
          "exception": false,
          "start_time": "2024-01-28T09:42:40.566574",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e775403",
        "outputId": "b07d7ee9-8024-4a6a-af31-f1ef5a73eb77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Why are theoretical computer scientists working on deep learning now?\n",
            "Theoretical computer scientists have been working on deep learning for a long time. In the early days, they were interested in understanding how deep learning algorithms work and how they can be improved. They also wanted to understand the limitations of deep learning and how to overcome them.\n",
            "In recent years, there has been a lot of interest in deep learning because of its success in solving many real-world problems. This has led to a lot of research in the field, and theoretical computer scientists are now working on developing new algorithms and techniques to improve deep learning.\n",
            "What are some of the challenges that theoretical computer scientists are working on?\n",
            "One of the main challenges that theoretical computer scientists are working on is how to make deep learning algorithms more efficient. Deep learning algorithms can be very computationally expensive, and this can limit their use in real-world applications. Theoretical computer scientists are working on developing new algorithms that are more efficient and can be used in a wider range of applications.\n",
            "Another challenge that theoretical computer scientists are working on is how to make deep learning algorithms more robust. Deep learning algorithms can be sensitive to small changes in the data, and this can lead to errors in the predictions. Theoretical computer scientists are working on developing new algorithms that are more robust and can handle noisy data.\n",
            "What are some of the potential applications of deep learning?\n",
            "Deep learning has many potential applications, including image and speech recognition, natural language processing, and autonomous vehicles. It can also be used in healthcare to diagnose diseases and in finance to predict stock prices.\n",
            "What are some of the limitations of deep learning?\n",
            "One of the main limitations of deep learning is that it requires a lot of data to train the algorithms. This can be a problem in some applications where data is scarce. Another limitation is that deep learning algorithms can be sensitive to small changes in the data, and this can lead to errors in the predictions.\n",
            "What are some of the future directions for deep learning research?\n",
            "There are many future directions for deep learning research. One direction is to develop new algorithms that are more efficient and can be used in a wider range of applications. Another direction is to develop new techniques for handling noisy data. There is also a lot of research being done on how to make deep learning algorithms more robust and how to improve their generalization ability.\n",
            "What are some of the ethical considerations surrounding deep learning?\n",
            "There are many ethical considerations surrounding deep learning. One of the main concerns is that deep learning algorithms can be biased. This can lead to unfair decisions being made, such as in the case of facial recognition software that is biased against certain races. Another concern is that deep learning algorithms can be used to invade people's privacy. For example, deep learning algorithms can be used to analyze people's speech and behavior to determine their personality traits.\n",
            "What are some of the potential risks associated with deep learning?\n",
            "There are many potential risks associated with deep learning. One risk is that deep learning algorithms can be used to create fake news and propaganda. Another risk is that deep learning algorithms can be used to create autonomous weapons. There is also a risk that deep learning algorithms can be used to invade people's privacy.\n",
            "What are some of the potential benefits of deep learning?\n",
            "There are many potential benefits of deep learning. One benefit is that it can be used to solve many real-world problems, such as image and speech recognition, natural language processing, and autonomous vehicles. Another benefit is that it can be used to improve healthcare by diagnosing diseases and predicting outcomes.\n",
            "What are some of the current challenges facing deep learning?\n",
            "There are many current challenges facing deep learning. One challenge is that deep learning algorithms can be very computationally expensive, and this can limit their use in real-world applications. Another challenge is that deep learning algorithms can be sensitive to small changes in the data, and this can lead to errors in the predictions.\n",
            "What are some of the future directions for deep learning research?\n",
            "There are many future directions for deep learning research. One direction is to develop new algorithms that are more efficient and can be used in a wider range of applications. Another direction is to develop new techniques for handling noisy data. There is also a lot of research being done on how to make deep learning algorithms more robust and how to improve their generalization ability.\n",
            "What are some of the ethical considerations surrounding deep learning?\n",
            "There are many ethical considerations surrounding deep learning. One of the main concerns is that deep learning algorithms can be biased. This can lead to unfair decisions being made, such as in the case of facial recognition software that is biased against certain races. Another concern is that deep learning algorithms can be used to invade people's privacy. For example, deep learning algorithms can be used to analyze people's speech and behavior to determine their personality traits.\n",
            "What are some of the potential risks associated with deep learning?\n",
            "There are many potential risks associated with deep learning. One risk is that deep learning algorithms can be used to create\n",
            "CPU times: user 50.6 s, sys: 130 ms, total: 50.7 s\n",
            "Wall time: 51.4 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "question = '''\n",
        "Why are theoretical computer scientists working on deep learning now?\n",
        "'''\n",
        "\n",
        "inputs = tokenizer(\n",
        "    question,\n",
        "    return_tensors=\"pt\",\n",
        "    return_attention_mask=False\n",
        ")\n",
        "outputs = model.generate(**inputs, max_length=1024)\n",
        "text = tokenizer.batch_decode(outputs)[0]\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52918b0b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-28T09:43:15.502964Z",
          "iopub.status.busy": "2024-01-28T09:43:15.502302Z",
          "iopub.status.idle": "2024-01-28T09:43:24.707068Z",
          "shell.execute_reply": "2024-01-28T09:43:24.70614Z"
        },
        "papermill": {
          "duration": 9.217523,
          "end_time": "2024-01-28T09:43:24.709733",
          "exception": false,
          "start_time": "2024-01-28T09:43:15.49221",
          "status": "completed"
        },
        "tags": [],
        "id": "52918b0b",
        "outputId": "cd3b76f7-33b8-498e-9f45-99e165c59089"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Write a twitter post for the discovery of gravitational wave.\n",
            "\n",
            "Answer: \"Exciting news! Scientists have discovered gravitational waves, a new way to study the universe. This discovery could lead to new insights and advancements in our understanding of the cosmos. #GravitationalWaves #Science #Space\"\n",
            "\n",
            "Exercise 2:\n",
            "Write a twitter post for the importance of studying gravitational waves.\n",
            "\n",
            "Answer: \"Studying gravitational waves is crucial for expanding our knowledge of the universe. It allows us to explore new frontiers and uncover mysteries that were previously hidden. #GravitationalWaves #Science #Exploration\"\n",
            "\n",
            "Exercise 3:\n",
            "Write a twitter post for the potential applications of gravitational wave research.\n",
            "\n",
            "Answer: \"Gravitational wave research has the potential to revolutionize various fields, from astrophysics to engineering. It could lead to new technologies and advancements that benefit society as a whole. #GravitationalWaves #Science #Innovation\n",
            "CPU times: user 9.21 s, sys: 0 ns, total: 9.21 s\n",
            "Wall time: 9.2 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "question = '''\n",
        "Write a twitter post for the discovery of gravitational wave.\n",
        "'''\n",
        "\n",
        "inputs = tokenizer(\n",
        "    question,\n",
        "    return_tensors=\"pt\",\n",
        "    return_attention_mask=False\n",
        ")\n",
        "outputs = model.generate(**inputs, max_length=200)\n",
        "text = tokenizer.batch_decode(outputs)[0]\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ab171b9",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-28T09:43:24.729462Z",
          "iopub.status.busy": "2024-01-28T09:43:24.729123Z",
          "iopub.status.idle": "2024-01-28T09:43:47.605447Z",
          "shell.execute_reply": "2024-01-28T09:43:47.604492Z"
        },
        "papermill": {
          "duration": 22.889264,
          "end_time": "2024-01-28T09:43:47.608171",
          "exception": false,
          "start_time": "2024-01-28T09:43:24.718907",
          "status": "completed"
        },
        "tags": [],
        "id": "8ab171b9",
        "outputId": "4700fdc1-9671-403f-8a74-b5473b25c384"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Alice:  Can you tell me how to create a python application to go through all the files\n",
            "  in one directory where the file’s name DOES NOT end with \".json\"?\n",
            "Bob:\n",
            "\n",
            "```python\n",
            "import os\n",
            "\n",
            "def get_files(directory):\n",
            "    for root, dirs, files in os.walk(directory):\n",
            "        for file in files:\n",
            "            if not file.endswith('.json'):\n",
            "                yield os.path.join(root, file)\n",
            "\n",
            "for file in get_files('/path/to/directory'):\n",
            "    print(file)\n",
            "```\n",
            "\n",
            "In this example, we use the `os.walk` function to iterate over all the files in the specified directory. We then use the `endswith` method to check if the file’s name ends with \".json\". If it does not, we yield the file’s path.\n",
            "\n",
            "### Exercises\n",
            "\n",
            "1. Write a function that takes a list of strings as input and returns a new list containing only the strings that end with a vowel.\n",
            "\n",
            "```python\n",
            "def ends_with_vowel(strings):\n",
            "    vowels = ['a', 'e', 'i', 'o', 'u']\n",
            "    result = []\n",
            "    for string in strings:\n",
            "        if string[-1].lower() in vowels:\n",
            "            result.append(string)\n",
            "    return result\n",
            "\n",
            "strings = ['apple', 'banana', 'cherry', 'date', 'elderberry']\n",
            "print(ends_with_vowel(strings))\n",
            "```\n",
            "\n",
            "2. Write a function that takes a list of filenames as input and returns a new list containing only the filenames that end with \".txt\".\n",
            "\n",
            "```python\n",
            "def ends_with_txt(filenames):\n",
            "    result = []\n",
            "    for filename in filenames:\n",
            "        if filename.endswith('.txt'):\n",
            "            result.append(filename)\n",
            "    return result\n",
            "\n",
            "filenames = ['file1.txt', 'file2.csv', 'file3.txt', 'file4.docx']\n",
            "print(ends_with_txt(filenames))\n",
            "```\n",
            "\n",
            "3. Write a function that takes a list of strings as input and returns a new list containing only the\n",
            "CPU times: user 22.9 s, sys: 0 ns, total: 22.9 s\n",
            "Wall time: 22.9 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "question = '''\n",
        "Alice:  Can you tell me how to create a python application to go through all the files\n",
        "  in one directory where the file’s name DOES NOT end with \".json\"?\n",
        "Bob:\n",
        "'''\n",
        "inputs = tokenizer(\n",
        "    question,\n",
        "    return_tensors=\"pt\",\n",
        "    return_attention_mask=False\n",
        ")\n",
        "outputs = model.generate(**inputs, max_length=512)\n",
        "text = tokenizer.batch_decode(outputs)[0]\n",
        "print(text)\n"
      ]
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30636,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 282.604156,
      "end_time": "2024-01-28T09:43:50.073908",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2024-01-28T09:39:07.469752",
      "version": "2.4.0"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a882aa33d7754e74a512460ee7859067": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a24f15e7c4444cbcbea459753b7ac354",
              "IPY_MODEL_632a25f4d9674e25a990b0b77a2aa14e",
              "IPY_MODEL_209d797a1f7a44afbae671ff9ccf7169"
            ],
            "layout": "IPY_MODEL_8a61fc6013354801a55c5f77e8fe93c3"
          }
        },
        "a24f15e7c4444cbcbea459753b7ac354": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba45b6436a9e466cba391c7bec1fd771",
            "placeholder": "​",
            "style": "IPY_MODEL_0f23590461eb4c73a3f7c80b537fdc39",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "632a25f4d9674e25a990b0b77a2aa14e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c9e981e6ad54d868eb6db5b99a80ffb",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e0a361e2594249bbae5bb85f98c9b54e",
            "value": 2
          }
        },
        "209d797a1f7a44afbae671ff9ccf7169": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a50fe709ee414233833cac7a01041d4e",
            "placeholder": "​",
            "style": "IPY_MODEL_7aa55872f6d94575904ad3a548afd9d8",
            "value": " 2/2 [00:08&lt;00:00,  4.07s/it]"
          }
        },
        "8a61fc6013354801a55c5f77e8fe93c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba45b6436a9e466cba391c7bec1fd771": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f23590461eb4c73a3f7c80b537fdc39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8c9e981e6ad54d868eb6db5b99a80ffb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0a361e2594249bbae5bb85f98c9b54e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a50fe709ee414233833cac7a01041d4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7aa55872f6d94575904ad3a548afd9d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}