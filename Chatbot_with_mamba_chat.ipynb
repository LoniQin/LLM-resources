{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM/4W2abHUE2Co7/PPxGGOq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LoniQin/LLM-resources/blob/main/Chatbot_with_mamba_chat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chatbot with mamba-chat"
      ],
      "metadata": {
        "id": "aAYbaX2DB9DJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. First download and install repository https://github.com/havenhq/mamba-chat.git. If you are in Google Colab, you may run following command to avoid issue:\n",
        "```\n",
        "!echo /usr/lib64-nvidia/ >/etc/ld.so.conf.d/libcuda.conf; ldconfig\n",
        "```"
      ],
      "metadata": {
        "id": "pt0oLmucB_jl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/havenhq/mamba-chat.git\n",
        "!pip install -r mamba-chat/requirements.txt\n",
        "!echo /usr/lib64-nvidia/ >/etc/ld.so.conf.d/libcuda.conf; ldconfig"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0dDNLVOCB0Y",
        "outputId": "afe3393f-a5eb-42a2-f164-283e3786e7c3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'mamba-chat'...\n",
            "remote: Enumerating objects: 113, done.\u001b[K\n",
            "remote: Counting objects: 100% (61/61), done.\u001b[K\n",
            "remote: Compressing objects: 100% (15/15), done.\u001b[K\n",
            "remote: Total 113 (delta 57), reused 46 (delta 46), pack-reused 52\u001b[K\n",
            "Receiving objects: 100% (113/113), 730.12 KiB | 14.04 MiB/s, done.\n",
            "Resolving deltas: 100% (58/58), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Open terminal, go to the downloaded github repository and input following command. Finally you can chat with Mamba in terminal.\n",
        "```\n",
        "python chat.py\n",
        "```"
      ],
      "metadata": {
        "id": "ILNPOFwtE2ni"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now I will download the model which is same with mamba-chat to have a conversation with."
      ],
      "metadata": {
        "id": "eV0dGBrgGw8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gc\n",
        "from transformers import AutoTokenizer\n",
        "from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel"
      ],
      "metadata": {
        "id": "xTBRTMLgHGvX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Mamba model"
      ],
      "metadata": {
        "id": "4qv6sN3QHJy9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\"\n",
        "model = MambaLMHeadModel.from_pretrained(\"havenhq/mamba-chat\", device=device, dtype=torch.float16)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"havenhq/mamba-chat\")\n",
        "eos = \"<|endoftext|>\"\n",
        "tokenizer.eos_token = eos\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.chat_template = AutoTokenizer.from_pretrained(\n",
        "  \"HuggingFaceH4/zephyr-7b-beta\"\n",
        ").chat_template"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rttO2ndjHMO8",
        "outputId": "a22c0fc1-e858-420c-e414-b2566b9c3aed"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_with_mamba(\n",
        "        user_message,\n",
        "        history: list[list[str]],\n",
        "        temperature: float = 0.9,\n",
        "        top_p: float = 0.7,\n",
        "        max_length: int = 2000,\n",
        "):\n",
        "        history_dict: list[dict[str, str]] = []\n",
        "        for user_m, assistant_m in history:\n",
        "            history_dict.append(dict(role=\"user\", content=user_m))\n",
        "            history_dict.append(dict(role=\"assistant\", content=assistant_m))\n",
        "        history_dict.append(dict(role=\"user\", content=user_message))\n",
        "\n",
        "        input_ids = tokenizer.apply_chat_template(\n",
        "            history_dict, return_tensors=\"pt\", add_generation_prompt=True\n",
        "        ).to(device)\n",
        "\n",
        "        out = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            max_length=max_length,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "        decoded = tokenizer.batch_decode(out)\n",
        "        assistant_message = (\n",
        "            decoded[0].split(\"<|assistant|>\\n\")[-1].replace(eos, \"\")\n",
        "        )\n",
        "        return assistant_message"
      ],
      "metadata": {
        "id": "no4WOS4THyM3"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversations = []\n",
        "def chat(question):\n",
        "  answer = chat_with_mamba(question, conversations)\n",
        "  conversations.append((question, answer))\n",
        "  print(answer)"
      ],
      "metadata": {
        "id": "AOccSPlmH0uW"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat(\"Hi, John. How are you?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkjC0UhJKD4W",
        "outputId": "d5a7c938-61d3-4db8-ad82-33fcdebf720a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am doing well, thank you for asking. How about you?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat(\"I feel really unhappy.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIOycs_IKIqG",
        "outputId": "4157bb4d-cef9-4e46-fc43-35c90bafa5fa"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am sorry to hear that. Can you please tell me more about your unhappiness?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat(\"New year is comming. I don't know how to make a plan for it.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDalQIdEKMKz",
        "outputId": "1bc02d01-111e-4ed8-98df-bc9cb29ca6a3"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I can understand your frustration. It's normal to feel overwhelmed and uncertain about what to do during the new year. However, there are many things you can do to make your new year more successful. Here are some tips:\n",
            "\n",
            "1. Set SMART goals: SMART goals are specific, measurable, achievable, realistic, and time-bound. They help you set clear and achievable goals for the new year.\n",
            "\n",
            "2. Set a budget: Make a budget for your new year's goals. This will help you stay within your limits and avoid overspending.\n",
            "\n",
            "3. Set reminders: Set reminders on your phone or calendar to remind you of your goals and tasks.\n",
            "\n",
            "4. Set aside time: Set aside a specific time each day to work on your goals. This will help you stay focused and avoid distractions.\n",
            "\n",
            "5. Seek support: Seek support from friends, family, or a coach to help you stay motivated and focused.\n",
            "\n",
            "6. Celebrate your successes: Celebrate your successes and acknowledge your progress. This will help you stay motivated and keep you motivated.\n",
            "\n",
            "Remember, the new year is a fresh start, and you can make a positive change in your life.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat(\"Thank you very much. I think I am more eager to lose weight. Could you give some suggestions?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1BozBchKdPA",
        "outputId": "e4a6ed82-e1ac-4e93-ad18-eb026b4506d7"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sure, here are some suggestions to help you lose weight:\n",
            "\n",
            "1. Set a realistic goal: Set a realistic goal for your weight loss. Don't aim for unrealistic weight loss goals that are too difficult to achieve. Instead, aim for a healthy weight loss of 1-2 pounds per week.\n",
            "\n",
            "2. Track your progress: Track your progress regularly by using a weight loss app or a food diary. This will help you stay motivated and track your progress.\n",
            "\n",
            "3. Eat healthy: Eat a balanced diet that includes lean proteins, fruits, vegetables, and whole grains. Avoid processed foods and sugar.\n",
            "\n",
            "4. Exercise regularly: Exercise regularly to burn calories and boost your metabolism. Choose activities that you enjoy and that you can sustain for a long time.\n",
            "\n",
            "5. Stay hydrated: Drink plenty of water throughout the day to keep your body hydrated and to help you feel full.\n",
            "\n",
            "6. Avoid sugar: Avoid sugar and processed foods that are high in sugar. Instead, choose whole foods that are low in sugar.\n",
            "\n",
            "Remember, weight loss is a journey, not a destination. It takes time and effort, but with patience and determination, you can achieve your weight loss goals.\n"
          ]
        }
      ]
    }
  ]
}